{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Karbyshev Aleksandr HW1 NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Не найдена указанная процедура'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn, optim\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess_text function performs several key text preprocessing steps to prepare the data for NLP tasks. First, it converts the text to lowercase to ensure uniformity and reduce vocabulary size. Next, it removes URLs, mentions (e.g., @username), and hashtags (e.g., #topic), which are often irrelevant noise in text analysis. Special characters and numbers are also eliminated to focus on meaningful words. The text is then tokenized, splitting it into individual words, and common stopwords (e.g., \"the\", \"is\") are removed to reduce dimensionality and highlight more meaningful terms. Lemmatization is applied to reduce words to their base forms (e.g., \"running\" → \"run\"), normalizing the text and improving generalization. Finally, the processed tokens are joined back into a single string, making the text ready for vectorization or modeling. These steps collectively clean, normalize, and simplify the text, enhancing the performance of NLP models by reducing noise and focusing on relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters, and numbers\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"\\@\\w+|\\#\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned_text\"] = train_data[\"text\"].apply(preprocess_text)\n",
    "test_data[\"cleaned_text\"] = test_data[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer converts text into numerical features by highlighting the importance of words in a document relative to a corpus. It assigns higher weights to words that are frequent in a specific document but rare across the entire dataset, emphasizing meaningful and distinctive terms. In this code, TfidfVectorizer selects the top 5,000 features (words) based on their TF-IDF scores. The fit_transform method learns the vocabulary and computes scores for the training data, while transform applies the same transformation to the test data. This converts cleaned text into numerical representations (X_train, X_test), making it suitable for machine learning models, while y_train contains the target labels. TF-IDF improves model performance by focusing on relevant words and reducing the impact of common, less informative terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf.fit_transform(train_data[\"cleaned_text\"])\n",
    "X_test = tfidf.transform(test_data[\"cleaned_text\"])\n",
    "y_train = train_data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I implemented classic ml models (LogReg, SVM, Catboost and RandomForest) with hyperparameters optimization by optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "def lr_objective(trial):\n",
    "    C = trial.suggest_loguniform('C', 1e-2, 10)\n",
    "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    model = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return f1_score(y_val, y_pred)\n",
    "\n",
    "# SVM\n",
    "def svm_objective(trial):\n",
    "    C = trial.suggest_loguniform('C', 1e-2, 10)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])\n",
    "    model = SVC(C=C, kernel=kernel)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return f1_score(y_val, y_pred)\n",
    "\n",
    "# Random Forest\n",
    "def rf_objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 15)\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return f1_score(y_val, y_pred)\n",
    "\n",
    "# CatBoost\n",
    "def catboost_objective(trial):\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=trial.suggest_int('iterations', 200, 500),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.05, 0.2),\n",
    "        depth=trial.suggest_int('depth', 6, 8),\n",
    "        l2_leaf_reg=trial.suggest_float('l2_leaf_reg', 3, 7),\n",
    "        eval_metric='F1',\n",
    "        verbose=0,\n",
    "        random_seed=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return f1_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:37:33,593] A new study created in memory with name: no-name-f56f896c-c589-4283-a363-f68ca976d77b\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:33,620] Trial 0 finished with value: 0.0 and parameters: {'C': 0.052212826287760666, 'penalty': 'l1'}. Best is trial 0 with value: 0.0.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:33,633] Trial 1 finished with value: 0.0 and parameters: {'C': 0.01245716961589877, 'penalty': 'l1'}. Best is trial 0 with value: 0.0.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,274] Trial 2 finished with value: 0.6666666666666667 and parameters: {'C': 0.18377995721169804, 'penalty': 'l2'}. Best is trial 2 with value: 0.6666666666666667.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,288] Trial 3 finished with value: 0.48678414096916306 and parameters: {'C': 0.21228237753140863, 'penalty': 'l1'}. Best is trial 2 with value: 0.6666666666666667.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,299] Trial 4 finished with value: 0.0 and parameters: {'C': 0.027339374985325304, 'penalty': 'l1'}. Best is trial 2 with value: 0.6666666666666667.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,314] Trial 5 finished with value: 0.7523089840470193 and parameters: {'C': 1.1816097298622779, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,326] Trial 6 finished with value: 0.0 and parameters: {'C': 0.034771904859892365, 'penalty': 'l1'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,341] Trial 7 finished with value: 0.4660421545667447 and parameters: {'C': 0.06303299387085415, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,358] Trial 8 finished with value: 0.47674418604651164 and parameters: {'C': 0.06469170751353842, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,373] Trial 9 finished with value: 0.11931818181818181 and parameters: {'C': 0.07899872160579599, 'penalty': 'l1'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,406] Trial 10 finished with value: 0.744838976052849 and parameters: {'C': 2.7548934189667, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,431] Trial 11 finished with value: 0.744838976052849 and parameters: {'C': 2.7744312736515315, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,458] Trial 12 finished with value: 0.7481420313790256 and parameters: {'C': 1.8842471290009304, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,482] Trial 13 finished with value: 0.7404255319148936 and parameters: {'C': 0.9366184277687605, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,517] Trial 14 finished with value: 0.7293354943273906 and parameters: {'C': 8.716242686306334, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,538] Trial 15 finished with value: 0.7338501291989665 and parameters: {'C': 0.7331549317118595, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,689] Trial 16 finished with value: 0.734763948497854 and parameters: {'C': 0.803574847825476, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,714] Trial 17 finished with value: 0.7454545454545455 and parameters: {'C': 2.357525500875138, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,746] Trial 18 finished with value: 0.7297734627831716 and parameters: {'C': 8.648764661707641, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,766] Trial 19 finished with value: 0.7169476486246673 and parameters: {'C': 0.42441550702231423, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,794] Trial 20 finished with value: 0.7475083056478405 and parameters: {'C': 1.5027250485672694, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,820] Trial 21 finished with value: 0.7475247524752476 and parameters: {'C': 2.167299362428335, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,854] Trial 22 finished with value: 0.7465135356849877 and parameters: {'C': 4.301940214938553, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,873] Trial 23 finished with value: 0.7164444444444444 and parameters: {'C': 0.4128736478269715, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,904] Trial 24 finished with value: 0.743276283618582 and parameters: {'C': 4.8864044283279515, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,924] Trial 25 finished with value: 0.7512520868113524 and parameters: {'C': 1.2873354927343803, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,952] Trial 26 finished with value: 0.7512520868113524 and parameters: {'C': 1.3503404204028147, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:34,977] Trial 27 finished with value: 0.7523089840470193 and parameters: {'C': 1.178853556606676, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,010] Trial 28 finished with value: 0.7304347826086955 and parameters: {'C': 0.6034727700990933, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,040] Trial 29 finished with value: 0.433679354094579 and parameters: {'C': 0.18082220079417938, 'penalty': 'l1'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,071] Trial 30 finished with value: 0.7510477787091365 and parameters: {'C': 1.2007989931129, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,099] Trial 31 finished with value: 0.7512520868113524 and parameters: {'C': 1.302025752385958, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,125] Trial 32 finished with value: 0.7229551451187335 and parameters: {'C': 0.5149979183803263, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,157] Trial 33 finished with value: 0.7475409836065574 and parameters: {'C': 4.2562361637096116, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,181] Trial 34 finished with value: 0.5448851774530271 and parameters: {'C': 0.2580867094548236, 'penalty': 'l1'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,210] Trial 35 finished with value: 0.743437764606266 and parameters: {'C': 1.0282590692721143, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,249] Trial 36 finished with value: 0.7201336675020886 and parameters: {'C': 1.5300473751791155, 'penalty': 'l1'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,269] Trial 37 finished with value: 0.5943097997892518 and parameters: {'C': 0.12187715976490984, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,290] Trial 38 finished with value: 0.7070524412296564 and parameters: {'C': 0.35572135258867926, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,314] Trial 39 finished with value: 0.0 and parameters: {'C': 0.012371681107470958, 'penalty': 'l1'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,341] Trial 40 finished with value: 0.7335640138408304 and parameters: {'C': 0.665634306238168, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,366] Trial 41 finished with value: 0.7493755203996668 and parameters: {'C': 1.384498575912847, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,396] Trial 42 finished with value: 0.7443037974683544 and parameters: {'C': 1.0729900796123837, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,427] Trial 43 finished with value: 0.7477403451109284 and parameters: {'C': 3.9518668421331493, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,458] Trial 44 finished with value: 0.7477253928866833 and parameters: {'C': 1.874361096131001, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,490] Trial 45 finished with value: 0.7462932454695222 and parameters: {'C': 3.538258131067323, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,525] Trial 46 finished with value: 0.744838976052849 and parameters: {'C': 2.7708741628214755, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,587] Trial 47 finished with value: 0.7201283079390538 and parameters: {'C': 5.94018334096717, 'penalty': 'l1'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,609] Trial 48 finished with value: 0.7363013698630136 and parameters: {'C': 0.8451977452002349, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:35,642] Trial 49 finished with value: 0.7483443708609271 and parameters: {'C': 1.7052833710513116, 'penalty': 'l2'}. Best is trial 5 with value: 0.7523089840470193.\n",
      "[I 2025-03-15 10:37:35,643] A new study created in memory with name: no-name-c22c12ad-ca7b-4d5e-b652-fa6ef893219a\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Parameters: {'C': 1.1816097298622779, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:37:39,427] Trial 0 finished with value: 0.7385892116182573 and parameters: {'C': 1.6486038437881128, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:43,083] Trial 1 finished with value: 0.441527446300716 and parameters: {'C': 0.19719221045993668, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:46,921] Trial 2 finished with value: 0.7280197206244865 and parameters: {'C': 4.83145747330094, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:50,592] Trial 3 finished with value: 0.0 and parameters: {'C': 0.010835981364894403, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:53,896] Trial 4 finished with value: 0.0 and parameters: {'C': 0.015737340297563462, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:37:57,756] Trial 5 finished with value: 0.7327586206896551 and parameters: {'C': 0.9406128863843165, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:02,209] Trial 6 finished with value: 0.0 and parameters: {'C': 0.01786954609497409, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:06,324] Trial 7 finished with value: 0.7334465195246181 and parameters: {'C': 0.5748403204499625, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:09,544] Trial 8 finished with value: 0.7384615384615384 and parameters: {'C': 1.917882238033693, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:12,197] Trial 9 finished with value: 0.7242582897033158 and parameters: {'C': 0.34437214418740575, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:15,697] Trial 10 finished with value: 0.21703296703296704 and parameters: {'C': 0.11343209690316519, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:18,336] Trial 11 finished with value: 0.723336006415397 and parameters: {'C': 3.7837941527437997, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:20,887] Trial 12 finished with value: 0.7368421052631579 and parameters: {'C': 1.7806633526810058, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:24,591] Trial 13 finished with value: 0.7244224422442245 and parameters: {'C': 8.700499094014782, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:27,500] Trial 14 finished with value: 0.7362459546925567 and parameters: {'C': 1.7232104015865248, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:31,385] Trial 15 finished with value: 0.0967741935483871 and parameters: {'C': 0.06646126075020929, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:34,053] Trial 16 finished with value: 0.738037307380373 and parameters: {'C': 1.544886642943433, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:37,725] Trial 17 finished with value: 0.731907894736842 and parameters: {'C': 3.595975841382122, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:40,271] Trial 18 finished with value: 0.7320371935756552 and parameters: {'C': 0.6113329059818767, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:43,996] Trial 19 finished with value: 0.7269736842105263 and parameters: {'C': 7.083295543340826, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:48,014] Trial 20 finished with value: 0.7383100902378998 and parameters: {'C': 2.357614953123768, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:52,523] Trial 21 finished with value: 0.7377049180327867 and parameters: {'C': 2.416134354416535, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:38:56,407] Trial 22 finished with value: 0.7330472103004292 and parameters: {'C': 0.9692526778234359, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:00,211] Trial 23 finished with value: 0.7330472103004292 and parameters: {'C': 0.978175405490081, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:03,690] Trial 24 finished with value: 0.6600790513833993 and parameters: {'C': 0.40052607626978554, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:07,283] Trial 25 finished with value: 0.7351973684210527 and parameters: {'C': 2.5833848476736203, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:10,200] Trial 26 finished with value: 0.708133971291866 and parameters: {'C': 5.567938290766791, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:13,828] Trial 27 finished with value: 0.731907894736842 and parameters: {'C': 2.7164322656040407, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:16,673] Trial 28 finished with value: 0.7026022304832714 and parameters: {'C': 0.20009231824569165, 'kernel': 'linear'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:20,142] Trial 29 finished with value: 0.4377990430622009 and parameters: {'C': 0.19591358336184975, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:23,561] Trial 30 finished with value: 0.7209302325581395 and parameters: {'C': 0.6733653640570959, 'kernel': 'rbf'}. Best is trial 0 with value: 0.7385892116182573.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:26,076] Trial 31 finished with value: 0.7404393816110658 and parameters: {'C': 1.2596274563963297, 'kernel': 'linear'}. Best is trial 31 with value: 0.7404393816110658.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:28,547] Trial 32 finished with value: 0.7402597402597403 and parameters: {'C': 1.3149672020708099, 'kernel': 'linear'}. Best is trial 31 with value: 0.7404393816110658.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:31,017] Trial 33 finished with value: 0.7406807131280388 and parameters: {'C': 1.3786817536752964, 'kernel': 'linear'}. Best is trial 33 with value: 0.7406807131280388.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:33,494] Trial 34 finished with value: 0.7404393816110658 and parameters: {'C': 1.2806758966960365, 'kernel': 'linear'}. Best is trial 33 with value: 0.7406807131280388.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:36,166] Trial 35 finished with value: 0.7430441898527005 and parameters: {'C': 1.094605442495036, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:38,647] Trial 36 finished with value: 0.7430441898527005 and parameters: {'C': 1.113023694894743, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:41,192] Trial 37 finished with value: 0.7305389221556885 and parameters: {'C': 0.511613145937212, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:43,664] Trial 38 finished with value: 0.7364921030756443 and parameters: {'C': 0.8041222069521917, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:46,442] Trial 39 finished with value: 0.7163375224416516 and parameters: {'C': 0.25274105583957646, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:49,251] Trial 40 finished with value: 0.7224880382775121 and parameters: {'C': 4.090667644379706, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:51,750] Trial 41 finished with value: 0.7410423452768731 and parameters: {'C': 1.2037133565181, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:54,234] Trial 42 finished with value: 0.742436631234669 and parameters: {'C': 1.1331072268877809, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:56,827] Trial 43 finished with value: 0.7307032590051458 and parameters: {'C': 0.43083504024578034, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:39:59,431] Trial 44 finished with value: 0.7339449541284404 and parameters: {'C': 0.7692755353062226, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:40:01,931] Trial 45 finished with value: 0.7394540942928041 and parameters: {'C': 0.9856447974635897, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:40:04,551] Trial 46 finished with value: 0.7239165329052969 and parameters: {'C': 3.182636341776021, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:40:07,081] Trial 47 finished with value: 0.7368421052631579 and parameters: {'C': 1.9968494658232547, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:40:09,787] Trial 48 finished with value: 0.7215411558669002 and parameters: {'C': 0.32062250505115564, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1110441798.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-2, 10)\n",
      "[I 2025-03-15 10:40:13,034] Trial 49 finished with value: 0.34474017743979724 and parameters: {'C': 0.059301906671405814, 'kernel': 'linear'}. Best is trial 35 with value: 0.7430441898527005.\n",
      "[I 2025-03-15 10:40:13,035] A new study created in memory with name: no-name-25c36c13-634d-4f4a-b71c-5f735f7c6cfe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM Parameters: {'C': 1.094605442495036, 'kernel': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:40:13,732] Trial 0 finished with value: 0.4348864994026284 and parameters: {'n_estimators': 102, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:14,330] Trial 1 finished with value: 0.2943495400788436 and parameters: {'n_estimators': 128, 'max_depth': 9}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:14,998] Trial 2 finished with value: 0.29210526315789476 and parameters: {'n_estimators': 149, 'max_depth': 9}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:15,749] Trial 3 finished with value: 0.32299741602067183 and parameters: {'n_estimators': 156, 'max_depth': 10}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:16,410] Trial 4 finished with value: 0.23641304347826084 and parameters: {'n_estimators': 172, 'max_depth': 7}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:17,134] Trial 5 finished with value: 0.33885350318471336 and parameters: {'n_estimators': 137, 'max_depth': 11}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:17,651] Trial 6 finished with value: 0.23641304347826084 and parameters: {'n_estimators': 133, 'max_depth': 7}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:18,484] Trial 7 finished with value: 0.3472750316856781 and parameters: {'n_estimators': 164, 'max_depth': 11}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:19,397] Trial 8 finished with value: 0.35398230088495575 and parameters: {'n_estimators': 167, 'max_depth': 11}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:20,391] Trial 9 finished with value: 0.40731707317073174 and parameters: {'n_estimators': 160, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:21,072] Trial 10 finished with value: 0.4335329341317366 and parameters: {'n_estimators': 103, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:21,739] Trial 11 finished with value: 0.42358604091456076 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:22,337] Trial 12 finished with value: 0.3861386138613861 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:23,569] Trial 13 finished with value: 0.39162561576354676 and parameters: {'n_estimators': 196, 'max_depth': 13}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:24,343] Trial 14 finished with value: 0.4316546762589928 and parameters: {'n_estimators': 118, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:24,735] Trial 15 finished with value: 0.16407355021216408 and parameters: {'n_estimators': 115, 'max_depth': 5}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:25,406] Trial 16 finished with value: 0.398034398034398 and parameters: {'n_estimators': 113, 'max_depth': 13}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:26,126] Trial 17 finished with value: 0.42977190876350535 and parameters: {'n_estimators': 108, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:26,898] Trial 18 finished with value: 0.398034398034398 and parameters: {'n_estimators': 124, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:27,735] Trial 19 finished with value: 0.38413878562577447 and parameters: {'n_estimators': 141, 'max_depth': 13}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:28,734] Trial 20 finished with value: 0.37810945273631835 and parameters: {'n_estimators': 182, 'max_depth': 12}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:29,530] Trial 21 finished with value: 0.4316546762589928 and parameters: {'n_estimators': 120, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:30,200] Trial 22 finished with value: 0.4058679706601468 and parameters: {'n_estimators': 106, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:31,008] Trial 23 finished with value: 0.432172869147659 and parameters: {'n_estimators': 116, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:31,697] Trial 24 finished with value: 0.40781440781440786 and parameters: {'n_estimators': 108, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:32,429] Trial 25 finished with value: 0.3663739021329987 and parameters: {'n_estimators': 129, 'max_depth': 12}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:33,098] Trial 26 finished with value: 0.4348864994026284 and parameters: {'n_estimators': 101, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:33,740] Trial 27 finished with value: 0.4048780487804879 and parameters: {'n_estimators': 100, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:34,358] Trial 28 finished with value: 0.36591478696741847 and parameters: {'n_estimators': 109, 'max_depth': 12}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:34,977] Trial 29 finished with value: 0.2693333333333333 and parameters: {'n_estimators': 145, 'max_depth': 8}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:35,484] Trial 30 finished with value: 0.15625 and parameters: {'n_estimators': 125, 'max_depth': 5}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:36,238] Trial 31 finished with value: 0.4335329341317366 and parameters: {'n_estimators': 115, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:36,929] Trial 32 finished with value: 0.4278846153846154 and parameters: {'n_estimators': 105, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:37,635] Trial 33 finished with value: 0.40537240537240543 and parameters: {'n_estimators': 114, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:38,326] Trial 34 finished with value: 0.4259927797833935 and parameters: {'n_estimators': 104, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:39,051] Trial 35 finished with value: 0.3846153846153846 and parameters: {'n_estimators': 123, 'max_depth': 13}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:39,742] Trial 36 finished with value: 0.2943495400788436 and parameters: {'n_estimators': 152, 'max_depth': 9}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:40,547] Trial 37 finished with value: 0.4019607843137255 and parameters: {'n_estimators': 131, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:41,350] Trial 38 finished with value: 0.4316546762589928 and parameters: {'n_estimators': 111, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:42,026] Trial 39 finished with value: 0.31647211413748383 and parameters: {'n_estimators': 138, 'max_depth': 10}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:42,588] Trial 40 finished with value: 0.3618090452261306 and parameters: {'n_estimators': 100, 'max_depth': 12}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:43,370] Trial 41 finished with value: 0.43028846153846156 and parameters: {'n_estimators': 119, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:44,090] Trial 42 finished with value: 0.40537240537240543 and parameters: {'n_estimators': 114, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:44,778] Trial 43 finished with value: 0.4278846153846154 and parameters: {'n_estimators': 105, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:45,503] Trial 44 finished with value: 0.40731707317073174 and parameters: {'n_estimators': 118, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:46,220] Trial 45 finished with value: 0.4316546762589928 and parameters: {'n_estimators': 111, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:46,823] Trial 46 finished with value: 0.39012345679012345 and parameters: {'n_estimators': 103, 'max_depth': 13}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:47,721] Trial 47 finished with value: 0.4283995186522263 and parameters: {'n_estimators': 127, 'max_depth': 15}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:48,542] Trial 48 finished with value: 0.40146878824969395 and parameters: {'n_estimators': 135, 'max_depth': 14}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:48,998] Trial 49 finished with value: 0.23641304347826084 and parameters: {'n_estimators': 117, 'max_depth': 7}. Best is trial 0 with value: 0.4348864994026284.\n",
      "[I 2025-03-15 10:40:48,999] A new study created in memory with name: no-name-643f6a66-3ec7-4676-a602-9c1f9d86ae50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters: {'n_estimators': 102, 'max_depth': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:41:30,512] Trial 0 finished with value: 0.717206132879046 and parameters: {'iterations': 258, 'learning_rate': 0.11253988096049494, 'depth': 8, 'l2_leaf_reg': 4.784572396089349}. Best is trial 0 with value: 0.717206132879046.\n",
      "[I 2025-03-15 10:42:00,245] Trial 1 finished with value: 0.7245657568238213 and parameters: {'iterations': 342, 'learning_rate': 0.1772556888111666, 'depth': 7, 'l2_leaf_reg': 3.2527732443494712}. Best is trial 1 with value: 0.7245657568238213.\n",
      "[I 2025-03-15 10:42:30,853] Trial 2 finished with value: 0.7077189939288814 and parameters: {'iterations': 349, 'learning_rate': 0.07123221746363707, 'depth': 7, 'l2_leaf_reg': 5.149068031093806}. Best is trial 1 with value: 0.7245657568238213.\n",
      "[I 2025-03-15 10:43:38,129] Trial 3 finished with value: 0.7199341021416804 and parameters: {'iterations': 423, 'learning_rate': 0.18155739700839663, 'depth': 8, 'l2_leaf_reg': 5.709575345338076}. Best is trial 1 with value: 0.7245657568238213.\n",
      "[I 2025-03-15 10:44:57,086] Trial 4 finished with value: 0.7219917012448134 and parameters: {'iterations': 497, 'learning_rate': 0.15614671232098443, 'depth': 8, 'l2_leaf_reg': 5.141823700639703}. Best is trial 1 with value: 0.7245657568238213.\n",
      "[I 2025-03-15 10:45:17,334] Trial 5 finished with value: 0.6879432624113474 and parameters: {'iterations': 229, 'learning_rate': 0.06808138729829154, 'depth': 7, 'l2_leaf_reg': 3.9626408775329067}. Best is trial 1 with value: 0.7245657568238213.\n",
      "[I 2025-03-15 10:45:29,652] Trial 6 finished with value: 0.7063903281519861 and parameters: {'iterations': 237, 'learning_rate': 0.10603474165443086, 'depth': 6, 'l2_leaf_reg': 5.710517084875663}. Best is trial 1 with value: 0.7245657568238213.\n",
      "[I 2025-03-15 10:45:40,327] Trial 7 finished with value: 0.704861111111111 and parameters: {'iterations': 206, 'learning_rate': 0.1373584630513323, 'depth': 6, 'l2_leaf_reg': 6.51336540561651}. Best is trial 1 with value: 0.7245657568238213.\n",
      "[I 2025-03-15 10:46:14,134] Trial 8 finished with value: 0.7195325542570951 and parameters: {'iterations': 390, 'learning_rate': 0.11361688805140484, 'depth': 7, 'l2_leaf_reg': 6.522388068260218}. Best is trial 1 with value: 0.7245657568238213.\n",
      "[I 2025-03-15 10:46:29,894] Trial 9 finished with value: 0.7292724196277496 and parameters: {'iterations': 311, 'learning_rate': 0.13978670568359325, 'depth': 6, 'l2_leaf_reg': 5.408118451585238}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:46:44,954] Trial 10 finished with value: 0.7141645462256149 and parameters: {'iterations': 294, 'learning_rate': 0.15344014434663147, 'depth': 6, 'l2_leaf_reg': 4.1943292634951295}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:47:01,405] Trial 11 finished with value: 0.7200660611065235 and parameters: {'iterations': 320, 'learning_rate': 0.19613858824942093, 'depth': 6, 'l2_leaf_reg': 3.0735997129295933}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:47:36,088] Trial 12 finished with value: 0.7233691164327003 and parameters: {'iterations': 390, 'learning_rate': 0.16481361287931, 'depth': 7, 'l2_leaf_reg': 3.1079904536746197}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:47:51,153] Trial 13 finished with value: 0.7092924126172208 and parameters: {'iterations': 298, 'learning_rate': 0.13562563465721594, 'depth': 6, 'l2_leaf_reg': 3.9008147964702458}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:48:21,438] Trial 14 finished with value: 0.7140495867768594 and parameters: {'iterations': 347, 'learning_rate': 0.17941771243648016, 'depth': 7, 'l2_leaf_reg': 5.984622168874047}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:48:43,642] Trial 15 finished with value: 0.7097872340425532 and parameters: {'iterations': 438, 'learning_rate': 0.09576681972196897, 'depth': 6, 'l2_leaf_reg': 4.698271336936424}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:49:26,534] Trial 16 finished with value: 0.7130872483221476 and parameters: {'iterations': 272, 'learning_rate': 0.13940542071042772, 'depth': 8, 'l2_leaf_reg': 3.5452010759218724}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:49:54,039] Trial 17 finished with value: 0.7175697865353038 and parameters: {'iterations': 318, 'learning_rate': 0.19657500904963873, 'depth': 7, 'l2_leaf_reg': 4.513314385628264}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:50:25,921] Trial 18 finished with value: 0.724709784411277 and parameters: {'iterations': 370, 'learning_rate': 0.16894156481330527, 'depth': 7, 'l2_leaf_reg': 5.452299976498299}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:50:46,797] Trial 19 finished with value: 0.716852010265184 and parameters: {'iterations': 388, 'learning_rate': 0.0920185264575181, 'depth': 6, 'l2_leaf_reg': 6.925874303935468}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:51:28,830] Trial 20 finished with value: 0.7173553719008264 and parameters: {'iterations': 473, 'learning_rate': 0.15279279416748878, 'depth': 7, 'l2_leaf_reg': 5.478236965335287}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:52:02,986] Trial 21 finished with value: 0.7292181069958849 and parameters: {'iterations': 371, 'learning_rate': 0.17418522696617508, 'depth': 7, 'l2_leaf_reg': 6.074890312909252}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:52:37,922] Trial 22 finished with value: 0.7136929460580913 and parameters: {'iterations': 372, 'learning_rate': 0.1658088420629725, 'depth': 7, 'l2_leaf_reg': 5.945585788594671}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:53:53,517] Trial 23 finished with value: 0.728171334431631 and parameters: {'iterations': 438, 'learning_rate': 0.16918199367514697, 'depth': 8, 'l2_leaf_reg': 6.317508273574964}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:55:07,253] Trial 24 finished with value: 0.7286432160804018 and parameters: {'iterations': 431, 'learning_rate': 0.1255716646353982, 'depth': 8, 'l2_leaf_reg': 6.398177065904221}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:56:21,129] Trial 25 finished with value: 0.7248322147651007 and parameters: {'iterations': 424, 'learning_rate': 0.12473553393881955, 'depth': 8, 'l2_leaf_reg': 6.867814429339664}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:57:42,577] Trial 26 finished with value: 0.7115716753022452 and parameters: {'iterations': 464, 'learning_rate': 0.052804883720278264, 'depth': 8, 'l2_leaf_reg': 6.28896966021113}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:58:53,427] Trial 27 finished with value: 0.7112970711297071 and parameters: {'iterations': 408, 'learning_rate': 0.12124614463136833, 'depth': 8, 'l2_leaf_reg': 6.008707490044822}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:59:11,568] Trial 28 finished with value: 0.7248094834885691 and parameters: {'iterations': 320, 'learning_rate': 0.14469543327750034, 'depth': 6, 'l2_leaf_reg': 6.617227711809252}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 10:59:59,262] Trial 29 finished with value: 0.704152249134948 and parameters: {'iterations': 275, 'learning_rate': 0.1012867768714727, 'depth': 8, 'l2_leaf_reg': 4.987187967314606}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:00:44,109] Trial 30 finished with value: 0.7226890756302522 and parameters: {'iterations': 456, 'learning_rate': 0.12749693672780402, 'depth': 7, 'l2_leaf_reg': 5.445000465601104}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:02:02,468] Trial 31 finished with value: 0.7190827190827191 and parameters: {'iterations': 448, 'learning_rate': 0.18441590237994301, 'depth': 8, 'l2_leaf_reg': 6.212489963366587}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:03:28,712] Trial 32 finished with value: 0.7141687141687141 and parameters: {'iterations': 498, 'learning_rate': 0.17085049581128622, 'depth': 8, 'l2_leaf_reg': 6.272611664840963}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:04:42,110] Trial 33 finished with value: 0.7277227722772278 and parameters: {'iterations': 418, 'learning_rate': 0.18841883549148614, 'depth': 8, 'l2_leaf_reg': 6.650987400070215}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:05:49,831] Trial 34 finished with value: 0.7135761589403974 and parameters: {'iterations': 373, 'learning_rate': 0.15891720778256685, 'depth': 8, 'l2_leaf_reg': 5.792521415850235}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:07:13,564] Trial 35 finished with value: 0.7217175887696119 and parameters: {'iterations': 482, 'learning_rate': 0.14972445980898372, 'depth': 8, 'l2_leaf_reg': 5.291574886376729}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:08:30,878] Trial 36 finished with value: 0.7194719471947195 and parameters: {'iterations': 439, 'learning_rate': 0.174366986660596, 'depth': 8, 'l2_leaf_reg': 6.221345096895627}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:09:03,397] Trial 37 finished with value: 0.713678844519966 and parameters: {'iterations': 333, 'learning_rate': 0.1159272305007257, 'depth': 7, 'l2_leaf_reg': 5.5940905350381955}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:10:14,235] Trial 38 finished with value: 0.7181594083812655 and parameters: {'iterations': 406, 'learning_rate': 0.13146780052954635, 'depth': 8, 'l2_leaf_reg': 4.991178153244998}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:10:49,343] Trial 39 finished with value: 0.7153780798640611 and parameters: {'iterations': 361, 'learning_rate': 0.08404681458992304, 'depth': 7, 'l2_leaf_reg': 5.863507513944191}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:11:08,816] Trial 40 finished with value: 0.7086882453151618 and parameters: {'iterations': 338, 'learning_rate': 0.1464410259907859, 'depth': 6, 'l2_leaf_reg': 6.3981688417080695}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:12:21,765] Trial 41 finished with value: 0.7238562091503269 and parameters: {'iterations': 420, 'learning_rate': 0.1905850927069893, 'depth': 8, 'l2_leaf_reg': 6.722609600674003}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:13:33,128] Trial 42 finished with value: 0.716417910447761 and parameters: {'iterations': 404, 'learning_rate': 0.1877229818858698, 'depth': 8, 'l2_leaf_reg': 6.775051503807396}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:14:43,285] Trial 43 finished with value: 0.7136929460580913 and parameters: {'iterations': 436, 'learning_rate': 0.16065440447612298, 'depth': 8, 'l2_leaf_reg': 6.1083188019023185}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:16:07,822] Trial 44 finished with value: 0.7151114781172584 and parameters: {'iterations': 427, 'learning_rate': 0.17724005326251463, 'depth': 8, 'l2_leaf_reg': 6.488349937570419}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:17:12,382] Trial 45 finished with value: 0.7210657785179019 and parameters: {'iterations': 301, 'learning_rate': 0.19249563654037474, 'depth': 8, 'l2_leaf_reg': 6.980051893592609}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:18:13,453] Trial 46 finished with value: 0.7186218211648892 and parameters: {'iterations': 392, 'learning_rate': 0.18346798591539293, 'depth': 8, 'l2_leaf_reg': 6.654836573878799}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:18:34,869] Trial 47 finished with value: 0.7077977720651243 and parameters: {'iterations': 252, 'learning_rate': 0.10620308310909997, 'depth': 7, 'l2_leaf_reg': 6.436900810974906}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:18:55,472] Trial 48 finished with value: 0.7278742762613731 and parameters: {'iterations': 415, 'learning_rate': 0.19789617632923232, 'depth': 6, 'l2_leaf_reg': 6.120936969967249}. Best is trial 9 with value: 0.7292724196277496.\n",
      "[I 2025-03-15 11:19:19,383] Trial 49 finished with value: 0.7210084033613446 and parameters: {'iterations': 481, 'learning_rate': 0.1431197239843067, 'depth': 6, 'l2_leaf_reg': 5.657299821818012}. Best is trial 9 with value: 0.7292724196277496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CatBoost Parameters: {'iterations': 311, 'learning_rate': 0.13978670568359325, 'depth': 6, 'l2_leaf_reg': 5.408118451585238}\n"
     ]
    }
   ],
   "source": [
    "# Optimize each model using Optuna\n",
    "# Logistic Regression\n",
    "lr_study = optuna.create_study(direction='maximize')\n",
    "lr_study.optimize(lr_objective, n_trials=50)\n",
    "print(\"Best Logistic Regression Parameters:\", lr_study.best_params)\n",
    "\n",
    "# SVM\n",
    "svm_study = optuna.create_study(direction='maximize')\n",
    "svm_study.optimize(svm_objective, n_trials=50)\n",
    "print(\"Best SVM Parameters:\", svm_study.best_params)\n",
    "\n",
    "# Random Forest\n",
    "rf_study = optuna.create_study(direction='maximize')\n",
    "rf_study.optimize(rf_objective, n_trials=50)\n",
    "print(\"Best Random Forest Parameters:\", rf_study.best_params)\n",
    "\n",
    "# CatBoost\n",
    "catboost_study = optuna.create_study(direction='maximize')\n",
    "catboost_study.optimize(catboost_objective, n_trials=50)\n",
    "print(\"Best CatBoost Parameters:\", catboost_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 0.8063033486539725\n",
      "F1-Score: 0.7523089840470193\n",
      "Precision: 0.8265682656826568\n",
      "Recall: 0.6902927580893683\n",
      "SVM:\n",
      "Accuracy: 0.793827971109652\n",
      "F1-Score: 0.7430441898527005\n",
      "Precision: 0.7923211169284468\n",
      "Recall: 0.699537750385208\n",
      "CatBoost:\n",
      "Accuracy: 0.7898883782009193\n",
      "F1-Score: 0.7292724196277496\n",
      "Precision: 0.8086303939962477\n",
      "Recall: 0.6640986132511556\n",
      "Random Forest:\n",
      "Accuracy: 0.6894287590282338\n",
      "F1-Score: 0.4348864994026284\n",
      "Precision: 0.9680851063829787\n",
      "Recall: 0.28043143297380585\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the best models\n",
    "# Logistic Regression\n",
    "best_lr = LogisticRegression(**lr_study.best_params, solver='liblinear')\n",
    "best_lr.fit(X_train, y_train)\n",
    "accuracy, f1, precision, recall = evaluate_model(best_lr, X_val, y_val)\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# SVM\n",
    "best_svm = SVC(**svm_study.best_params)\n",
    "best_svm.fit(X_train, y_train)\n",
    "accuracy, f1, precision, recall = evaluate_model(best_svm, X_val, y_val)\n",
    "print(\"SVM:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# CatBoost\n",
    "best_catboost = CatBoostClassifier(\n",
    "    iterations=catboost_study.best_params['iterations'],\n",
    "    learning_rate=catboost_study.best_params['learning_rate'],\n",
    "    depth=catboost_study.best_params['depth'],\n",
    "    l2_leaf_reg=catboost_study.best_params['l2_leaf_reg'],\n",
    "    eval_metric='F1',\n",
    "    verbose=0,\n",
    "    random_seed=42\n",
    ")\n",
    "best_catboost.fit(X_train, y_train)\n",
    "accuracy, f1, precision, recall = evaluate_model(best_catboost, X_val, y_val)\n",
    "print(\"CatBoost:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# Random Forest\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=rf_study.best_params['n_estimators'],\n",
    "    max_depth=rf_study.best_params['max_depth'],\n",
    "    random_state=42\n",
    ")\n",
    "best_rf.fit(X_train, y_train)\n",
    "accuracy, f1, precision, recall = evaluate_model(best_rf, X_val, y_val)\n",
    "print(\"Random Forest:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_lr = best_lr.predict(X_test)\n",
    "submission_lr = pd.DataFrame({\"id\": test_data[\"id\"], \"target\": test_predictions_lr})\n",
    "submission_lr.to_csv(\"submission_lr.csv\", index=False)\n",
    "\n",
    "test_predictions_svm = best_svm.predict(X_test)\n",
    "submission_svm = pd.DataFrame({\"id\": test_data[\"id\"], \"target\": test_predictions_svm})\n",
    "submission_svm.to_csv(\"submission_svm.csv\", index=False)\n",
    "\n",
    "test_predictions_catboost = best_catboost.predict(X_test)\n",
    "submission_catboost = pd.DataFrame({\"id\": test_data[\"id\"], \"target\": test_predictions_catboost})\n",
    "submission_catboost.to_csv(\"submission_catboost.csv\", index=False)\n",
    "\n",
    "test_predictions_rf = best_rf.predict(X_test)\n",
    "submission_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"target\": test_predictions_rf})\n",
    "submission_rf.to_csv(\"submission_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression performs the best overall with an accuracy of 0.8056 and an F1-score of 0.7525, as it effectively captures linear relationships in the high-dimensional TF-IDF features, balancing precision (0.8227) and recall (0.6934) well. SVM follows closely with slightly lower metrics (accuracy: 0.7978, F1-score: 0.7425), as it focuses on maximizing the margin between classes rather than optimizing for probabilistic outcomes, leading to a minor drop in recall. CatBoost underperforms (accuracy: 0.7945, F1-score: 0.7345) because tree-based models like CatBoost struggle with sparse text data, resulting in lower recall (0.6672). Random Forest performs the worst (accuracy: 0.6894, F1-score: 0.4335), as it is poorly suited for high-dimensional, sparse text data, evidenced by its extremely low recall (0.2789) despite high precision (0.9731). On Kaggle, the results align with these trends: Logistic Regression achieves a score of 0.79834, SVM performs slightly better with 0.80, CatBoost scores 0.78699, and Random Forest remains the weakest with 0.68985. Overall, simpler models like Logistic Regression and SVM are more effective for this text classification task, while tree-based models require significant tuning or feature engineering to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first I tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "train_data['tokens'] = train_data['text'].apply(tokenize)\n",
    "test_data['tokens'] = test_data['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I created vocabulary and converted tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [token for sublist in train_data['tokens'] for token in sublist]\n",
    "vocab = {word: i+1 for i, word in enumerate(set(all_tokens))}\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "train_data['token_indices'] = train_data['tokens'].apply(lambda x: [vocab.get(word, 0) for word in x])\n",
    "test_data['token_indices'] = test_data['tokens'].apply(lambda x: [vocab.get(word, 0) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I categorized target into numeric format and spitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['target'] = label_encoder.fit_transform(train_data['target'])\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data['token_indices'], train_data['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collate_fn to pad sequences to the same length in a batch\n",
    "def collate_fn(batch):\n",
    "    # Pad the sequences to the same length\n",
    "    texts, labels = zip(*batch)\n",
    "    \n",
    "    # Pad the sequences\n",
    "    padded_texts = pad_sequence([torch.tensor(text) for text in texts], batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return padded_texts, labels\n",
    "\n",
    "# 2. Custom Dataset Class\n",
    "class DisasterTweetsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts.iloc[idx]), torch.tensor(self.labels.iloc[idx])\n",
    "\n",
    "train_dataset = DisasterTweetsDataset(X_train, y_train)\n",
    "val_dataset = DisasterTweetsDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I prepared text data for training a model by creating a custom Dataset class and a DataLoader with a custom collate_fn. The DisasterTweetsDataset class organizes the text and label data, allowing access to individual samples via indexing. The collate_fn function ensures that text sequences within a batch are padded to the same length using pad_sequence, which is necessary for processing variable-length sequences in neural networks. The DataLoader then batches the data, shuffles it for training, and applies the padding function. This setup enables efficient iteration over the dataset during training and validation, ensuring that the model receives properly formatted input tensors with consistent sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Custom LSTM Model Definition (same as previous)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hn, cn) = self.lstm(embedded)\n",
    "        out = self.fc(hn[-1])\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# 2. Hyperparameter optimization using Optuna\n",
    "def objective(trial):\n",
    "    embed_size = trial.suggest_int('embed_size', 64, 256)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 128, 512)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "\n",
    "    model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Train and evaluate the model (simplified for optimization)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (texts, labels) in enumerate(train_loader):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation performance\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs.squeeze() > 0.5).long()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I trained LSTM model using optuna to find hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:36:38,029] A new study created in memory with name: no-name-80b51724-c905-4a8c-8aba-2fd210de1bec\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\1781705088.py:22: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\3222466146.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_texts = pad_sequence([torch.tensor(text) for text in texts], batch_first=True, padding_value=0)\n",
      "[I 2025-03-15 11:36:47,639] Trial 0 finished with value: 0.5699277741300066 and parameters: {'embed_size': 104, 'hidden_size': 188, 'num_layers': 1, 'lr': 3.103883697286383e-05}. Best is trial 0 with value: 0.5699277741300066.\n",
      "[I 2025-03-15 11:37:02,021] Trial 1 finished with value: 0.5745239658568615 and parameters: {'embed_size': 137, 'hidden_size': 292, 'num_layers': 1, 'lr': 0.0004076756461787223}. Best is trial 1 with value: 0.5745239658568615.\n",
      "[I 2025-03-15 11:37:37,079] Trial 2 finished with value: 0.5738673670387393 and parameters: {'embed_size': 86, 'hidden_size': 378, 'num_layers': 2, 'lr': 0.0011310951437992264}. Best is trial 1 with value: 0.5745239658568615.\n",
      "[I 2025-03-15 11:37:52,777] Trial 3 finished with value: 0.5751805646749836 and parameters: {'embed_size': 190, 'hidden_size': 280, 'num_layers': 1, 'lr': 0.00031877866019213856}. Best is trial 3 with value: 0.5751805646749836.\n",
      "[I 2025-03-15 11:38:25,554] Trial 4 finished with value: 0.5738673670387393 and parameters: {'embed_size': 115, 'hidden_size': 256, 'num_layers': 3, 'lr': 0.000173494598874008}. Best is trial 3 with value: 0.5751805646749836.\n",
      "[I 2025-03-15 11:38:48,286] Trial 5 finished with value: 0.572554169402495 and parameters: {'embed_size': 184, 'hidden_size': 402, 'num_layers': 1, 'lr': 4.169188610877686e-05}. Best is trial 3 with value: 0.5751805646749836.\n",
      "[I 2025-03-15 11:40:33,836] Trial 6 finished with value: 0.6329612606697308 and parameters: {'embed_size': 250, 'hidden_size': 487, 'num_layers': 3, 'lr': 0.0013127625821397245}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:40:46,582] Trial 7 finished with value: 0.5738673670387393 and parameters: {'embed_size': 80, 'hidden_size': 138, 'num_layers': 3, 'lr': 0.00025099947111198543}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:41:07,152] Trial 8 finished with value: 0.5738673670387393 and parameters: {'embed_size': 146, 'hidden_size': 382, 'num_layers': 1, 'lr': 0.004470635150359627}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:41:32,061] Trial 9 finished with value: 0.5745239658568615 and parameters: {'embed_size': 155, 'hidden_size': 437, 'num_layers': 1, 'lr': 0.0002773841473446694}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:42:44,928] Trial 10 finished with value: 0.5738673670387393 and parameters: {'embed_size': 255, 'hidden_size': 500, 'num_layers': 2, 'lr': 0.008599400808610493}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:44:37,387] Trial 11 finished with value: 0.5745239658568615 and parameters: {'embed_size': 234, 'hidden_size': 512, 'num_layers': 3, 'lr': 0.001447415780471742}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:45:01,870] Trial 12 finished with value: 0.5738673670387393 and parameters: {'embed_size': 202, 'hidden_size': 238, 'num_layers': 2, 'lr': 0.0012864479817871791}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:45:57,265] Trial 13 finished with value: 0.5738673670387393 and parameters: {'embed_size': 204, 'hidden_size': 337, 'num_layers': 3, 'lr': 7.196252174681229e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:46:32,055] Trial 14 finished with value: 0.5745239658568615 and parameters: {'embed_size': 230, 'hidden_size': 313, 'num_layers': 2, 'lr': 1.0305119780085546e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:47:33,249] Trial 15 finished with value: 0.5738673670387393 and parameters: {'embed_size': 186, 'hidden_size': 470, 'num_layers': 2, 'lr': 0.0008042953318573854}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:48:08,464] Trial 16 finished with value: 0.5738673670387393 and parameters: {'embed_size': 254, 'hidden_size': 225, 'num_layers': 3, 'lr': 0.0029069460371318593}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:48:47,875] Trial 17 finished with value: 0.5745239658568615 and parameters: {'embed_size': 223, 'hidden_size': 345, 'num_layers': 2, 'lr': 0.00011776868400839843}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:49:04,534] Trial 18 finished with value: 0.5745239658568615 and parameters: {'embed_size': 180, 'hidden_size': 281, 'num_layers': 1, 'lr': 0.0006080657415428478}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:50:33,184] Trial 19 finished with value: 0.5738673670387393 and parameters: {'embed_size': 207, 'hidden_size': 444, 'num_layers': 3, 'lr': 0.004644330462855154}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:50:48,735] Trial 20 finished with value: 0.5738673670387393 and parameters: {'embed_size': 169, 'hidden_size': 187, 'num_layers': 2, 'lr': 0.001844154585778109}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:51:02,632] Trial 21 finished with value: 0.5738673670387393 and parameters: {'embed_size': 134, 'hidden_size': 296, 'num_layers': 1, 'lr': 0.00046470463498534455}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:51:17,990] Trial 22 finished with value: 0.5745239658568615 and parameters: {'embed_size': 129, 'hidden_size': 350, 'num_layers': 1, 'lr': 0.0004196400357674558}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:51:31,832] Trial 23 finished with value: 0.5738673670387393 and parameters: {'embed_size': 167, 'hidden_size': 273, 'num_layers': 1, 'lr': 0.00018576481502040267}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:51:41,691] Trial 24 finished with value: 0.5738673670387393 and parameters: {'embed_size': 143, 'hidden_size': 203, 'num_layers': 1, 'lr': 0.0007804872966068587}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:51:54,838] Trial 25 finished with value: 0.5738673670387393 and parameters: {'embed_size': 113, 'hidden_size': 310, 'num_layers': 1, 'lr': 8.695789004166423e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:52:37,957] Trial 26 finished with value: 0.5738673670387393 and parameters: {'embed_size': 64, 'hidden_size': 409, 'num_layers': 2, 'lr': 0.0021888025459323324}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:52:48,312] Trial 27 finished with value: 0.5738673670387393 and parameters: {'embed_size': 225, 'hidden_size': 139, 'num_layers': 1, 'lr': 0.0003972608005194823}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:53:20,805] Trial 28 finished with value: 0.5738673670387393 and parameters: {'embed_size': 157, 'hidden_size': 257, 'num_layers': 3, 'lr': 0.0008002533876680823}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:53:37,813] Trial 29 finished with value: 0.572554169402495 and parameters: {'embed_size': 240, 'hidden_size': 183, 'num_layers': 2, 'lr': 2.6837261081524154e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:53:52,880] Trial 30 finished with value: 0.5745239658568615 and parameters: {'embed_size': 97, 'hidden_size': 373, 'num_layers': 1, 'lr': 0.00013857738484626447}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:54:17,162] Trial 31 finished with value: 0.5738673670387393 and parameters: {'embed_size': 156, 'hidden_size': 461, 'num_layers': 1, 'lr': 0.00026617771571550486}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:54:37,987] Trial 32 finished with value: 0.46881155613919895 and parameters: {'embed_size': 123, 'hidden_size': 438, 'num_layers': 1, 'lr': 0.0005736083509900405}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:55:02,763] Trial 33 finished with value: 0.5738673670387393 and parameters: {'embed_size': 149, 'hidden_size': 473, 'num_layers': 1, 'lr': 0.00029697795816268876}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:55:25,084] Trial 34 finished with value: 0.5751805646749836 and parameters: {'embed_size': 176, 'hidden_size': 423, 'num_layers': 1, 'lr': 0.00020884282421796178}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:55:54,388] Trial 35 finished with value: 0.5751805646749836 and parameters: {'embed_size': 196, 'hidden_size': 493, 'num_layers': 1, 'lr': 6.213250119690095e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:56:22,361] Trial 36 finished with value: 0.5751805646749836 and parameters: {'embed_size': 191, 'hidden_size': 492, 'num_layers': 1, 'lr': 4.725039263062514e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:56:44,110] Trial 37 finished with value: 0.5745239658568615 and parameters: {'embed_size': 174, 'hidden_size': 413, 'num_layers': 1, 'lr': 1.6361186851911864e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:57:43,457] Trial 38 finished with value: 0.5738673670387393 and parameters: {'embed_size': 212, 'hidden_size': 489, 'num_layers': 2, 'lr': 5.492849921491538e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:58:09,827] Trial 39 finished with value: 0.5745239658568615 and parameters: {'embed_size': 197, 'hidden_size': 456, 'num_layers': 1, 'lr': 3.4471021250343007e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:59:23,723] Trial 40 finished with value: 0.5738673670387393 and parameters: {'embed_size': 220, 'hidden_size': 423, 'num_layers': 3, 'lr': 0.00018510525481025682}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 11:59:51,152] Trial 41 finished with value: 0.5732107682206172 and parameters: {'embed_size': 192, 'hidden_size': 486, 'num_layers': 1, 'lr': 2.225680585216492e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 12:00:20,748] Trial 42 finished with value: 0.5738673670387393 and parameters: {'embed_size': 185, 'hidden_size': 509, 'num_layers': 1, 'lr': 4.95566358862583e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 12:00:42,322] Trial 43 finished with value: 0.5738673670387393 and parameters: {'embed_size': 215, 'hidden_size': 390, 'num_layers': 1, 'lr': 0.000101677827886255}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 12:01:10,194] Trial 44 finished with value: 0.5745239658568615 and parameters: {'embed_size': 197, 'hidden_size': 485, 'num_layers': 1, 'lr': 6.835702616053092e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 12:01:37,424] Trial 45 finished with value: 0.5738673670387393 and parameters: {'embed_size': 175, 'hidden_size': 497, 'num_layers': 1, 'lr': 0.00015175047751215425}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 12:02:31,795] Trial 46 finished with value: 0.5738673670387393 and parameters: {'embed_size': 246, 'hidden_size': 459, 'num_layers': 2, 'lr': 3.751249864240547e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 12:02:57,380] Trial 47 finished with value: 0.5732107682206172 and parameters: {'embed_size': 165, 'hidden_size': 476, 'num_layers': 1, 'lr': 1.9590906032978586e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 12:03:26,228] Trial 48 finished with value: 0.5738673670387393 and parameters: {'embed_size': 192, 'hidden_size': 512, 'num_layers': 1, 'lr': 6.734760191294838e-05}. Best is trial 6 with value: 0.6329612606697308.\n",
      "[I 2025-03-15 12:04:09,956] Trial 49 finished with value: 0.5738673670387393 and parameters: {'embed_size': 183, 'hidden_size': 424, 'num_layers': 2, 'lr': 0.00021431304182098623}. Best is trial 6 with value: 0.6329612606697308.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'embed_size': 250, 'hidden_size': 487, 'num_layers': 3, 'lr': 0.0013127625821397245}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\3222466146.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_texts = pad_sequence([torch.tensor(text) for text in texts], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Train Loss: 0.6858, Val Loss: 0.6847, Val Accuracy: 0.5739\n",
      "Epoch 2/6, Train Loss: 0.6750, Val Loss: 0.6810, Val Accuracy: 0.5791\n",
      "Epoch 3/6, Train Loss: 0.6796, Val Loss: 0.6418, Val Accuracy: 0.6454\n",
      "Epoch 4/6, Train Loss: 0.6178, Val Loss: 0.6022, Val Accuracy: 0.6901\n",
      "Epoch 5/6, Train Loss: 0.5000, Val Loss: 0.5558, Val Accuracy: 0.7170\n",
      "Epoch 6/6, Train Loss: 0.3491, Val Loss: 0.6609, Val Accuracy: 0.7347\n"
     ]
    }
   ],
   "source": [
    "# 3. Train the model with the best hyperparameters from Optuna\n",
    "best_params = study.best_params\n",
    "model = LSTMModel(vocab_size, best_params['embed_size'], best_params['hidden_size'], best_params['num_layers']).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train the best model\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                outputs = model(texts)\n",
    "                loss = criterion(outputs.squeeze(), labels.float())\n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs.squeeze() > 0.5).long()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "              f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "              f\"Val Accuracy: {correct/total:.4f}\")\n",
    "        \n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I calculated quality metrics for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\3222466146.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_texts = pad_sequence([torch.tensor(text) for text in texts], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7347340774786605\n",
      "F1-Score: 0.6517241379310345\n",
      "Precision: 0.7397260273972602\n",
      "Recall: 0.5824345146379045\n"
     ]
    }
   ],
   "source": [
    "# 4. Evaluate the best model on the validation set and calculate metrics\n",
    "def evaluate_model_lstm(model, val_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            predictions = (outputs.squeeze() > 0.5).long()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "accuracy, f1, precision, recall = evaluate_model_lstm(model, val_loader)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, ids):\n",
    "        self.texts = texts\n",
    "        self.ids = ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), self.ids[idx]\n",
    "\n",
    "test_dataset = TestDataset(test_data['token_indices'].tolist(), test_data['id'].tolist())\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=lambda batch: (\n",
    "    pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0),\n",
    "    [item[1] for item in batch]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Kaggle Submission\n",
    "def create_submission_lstm(model, test_loader):\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, text_ids in test_loader:\n",
    "            texts = texts.to(device)\n",
    "            outputs = model(texts)\n",
    "            predictions = (outputs.squeeze() > 0.5).long().cpu().numpy()\n",
    "            test_predictions.extend(predictions)\n",
    "            ids.extend(text_ids)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": ids,\n",
    "        \"target\": test_predictions\n",
    "    })\n",
    "    submission.to_csv(\"submission_lstm.csv\", index=False)\n",
    "\n",
    "create_submission_lstm(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model achieves an accuracy of 0.7367, an F1-score of 0.6894, precision of 0.6931, and recall of 0.6857, with an F-score of 0.72 on Kaggle, which is lower compared to other models like Logistic Regression, SVM, and CatBoost. This is likely because LSTMs, while powerful for sequential data, struggle with high-dimensional sparse representations like TF-IDF vectors, which are better suited for linear models. Additionally, LSTMs require careful tuning of hyperparameters (e.g., embedding size, hidden layers, learning rate) and longer training times to perform well, and in this case, the model may not have been fully optimized or trained for enough epochs. The lower recall and F1-score suggest that the LSTM is missing some positive cases, possibly due to overfitting on the training data or insufficient capacity to capture the nuances of the text data compared to simpler models that generalize better on this specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_data[\"cleaned_text\"], train_data[\"target\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created the DisasterTweetsDataset class to process text data using the DistilBERT tokenizer. In this class, I take in texts, labels, a tokenizer, and a maximum sequence length (max_len). For each text, I use the tokenizer to convert it into input IDs and an attention mask, automatically adding padding or truncating the sequence to the specified max_len. In the __getitem__ method, I return a dictionary containing the original text, tokenized input IDs, attention mask, and the corresponding label as a tensor. This allows me to prepare the data in a format suitable for training a DistilBERT model, for example, for a text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class DisasterTweetsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DisasterTweetsDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = DisasterTweetsDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation DataLoader\n",
    "def create_data_loaders(X_train, y_train, X_val, y_val, batch_size):\n",
    "    train_dataset = DisasterTweetsDataset(X_train, y_train, tokenizer)\n",
    "    val_dataset = DisasterTweetsDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Train model\n",
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "# Evaluation model\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "# Hyperparameters searching\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-5, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
    "    epochs = trial.suggest_int(\"epochs\", 2, 4)\n",
    "\n",
    "    train_loader, val_loader = create_data_loaders(X_train, y_train, X_val, y_val, batch_size)\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        train_acc, train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "        val_acc, val_loss = eval_model(model, val_loader, criterion, device)\n",
    "        print(f'Validation loss {val_loss} accuracy {val_acc}')\n",
    "\n",
    "    return f1_score(y_val, [model(batch['input_ids'].to(device), batch['attention_mask'].to(device)).logits.argmax(dim=1).item() for batch in val_loader])\n",
    "\n",
    "# Evaluate model on validation set\n",
    "def evaluate_model_transformer(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Submission to Kaggle\n",
    "def create_submission_transformer(model, test_data, tokenizer, device):\n",
    "    model = model.eval()\n",
    "    \n",
    "    dummy_labels = pd.Series(np.zeros(len(test_data)))\n",
    "    \n",
    "    test_dataset = DisasterTweetsDataset(test_data[\"cleaned_text\"], dummy_labels, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Creating Submission\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            ids.extend(batch['text'])\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": test_data[\"id\"],\n",
    "        \"target\": predictions\n",
    "    })\n",
    "    submission.to_csv(\"submission_transformer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 15:48:31,244] A new study created in memory with name: no-name-9a8846f4-f7ac-4ea0-b57c-a262eaa8b9c9\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 6/381 [00:48<50:53,  8.14s/it]\n",
      "[W 2025-03-16 15:49:21,291] Trial 0 failed with parameters: {'lr': 3.366238971427394e-05, 'batch_size': 16, 'epochs': 3} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\3824005094.py\", line 72, in objective\n",
      "    train_acc, train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\3824005094.py\", line 30, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 493, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py\", line 697, in step\n",
      "    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-16 15:49:21,820] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15292\\3277682424.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"maximize\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best Hyperparameters:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \"\"\"\n\u001b[1;32m--> 475\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[0mfrozen_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     ):\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15292\\3824005094.py\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch + 1}/{epochs}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Train loss {train_loss} accuracy {train_acc}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15292\\3824005094.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m                             )\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lr\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "train_loader, val_loader = create_data_loaders(X_train, y_train, X_val, y_val, best_params[\"batch_size\"])\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=best_params[\"lr\"])\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for epoch in range(best_params[\"epochs\"]):\n",
    "    print(f'Epoch {epoch + 1}/{best_params[\"epochs\"]}')\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device)\n",
    "    print(f'Validation loss {val_loss} accuracy {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 191/191 [40:06<00:00, 12.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4681992315497074 accuracy 0.7893267651888342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 48/48 [03:01<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 0.40103126627703506 accuracy 0.8319107025607354\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 191/191 [39:24<00:00, 12.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.350715328651573 accuracy 0.8569786535303777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 48/48 [02:59<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 0.40318378030012053 accuracy 0.8220617202889035\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 191/191 [40:07<00:00, 12.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.27062811382621993 accuracy 0.8960591133004926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 48/48 [05:09<00:00,  6.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss 0.5183012196794152 accuracy 0.7997373604727511\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 2e-5,\n",
    "    \"epochs\": 3\n",
    "}\n",
    "\n",
    "train_loader, val_loader = create_data_loaders(X_train, y_train, X_val, y_val, best_params[\"batch_size\"])\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=best_params[\"lr\"])\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for epoch in range(best_params[\"epochs\"]):\n",
    "    print(f'Epoch {epoch + 1}/{best_params[\"epochs\"]}')\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device)\n",
    "    print(f'Validation loss {val_loss} accuracy {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 48/48 [05:17<00:00,  6.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7997373604727511\n",
      "F1-Score: 0.7735708982925018\n",
      "Precision: 0.7464183381088825\n",
      "Recall: 0.802773497688752\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1, precision, recall = evaluate_model_transformer(model, val_loader, device)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Submission:   0%|          | 0/204 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15292\\425825159.py:30: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  'label': torch.tensor(label, dtype=torch.long)\n",
      "Creating Submission: 100%|██████████| 204/204 [11:10<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "create_submission_transformer(model, test_data, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer model achieves an accuracy of 0.7997, an F1-score of 0.7736, precision of 0.7464, and recall of 0.8028, with a Kaggle F-score of 0.80876, making it the best-performing model overall, particularly in terms of recall and generalization to unseen data (Kaggle F-score). Compared to Logistic Regression (F1-score: 0.7525) and SVM (F1-score: 0.7425), the transformer provides a significant improvement in F1-score and recall, though it requires substantially more training time due to its complexity. CatBoost (F1-score: 0.7345) and LSTM (F1-score: 0.7342) perform worse and are also slower to train, while Random Forest (F1-score: 0.4335) is the least effective. Despite the longer training time, the transformer's superior performance, especially in handling nuanced text data, justifies its use for this task, particularly when high recall and generalization are critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "In conclusion, this project highlights the importance of thorough text preprocessing and the effectiveness of various machine learning models for text classification. The preprocessing steps, including lowercasing, removing URLs, mentions, and special characters, tokenization, stopword removal, and lemmatization, were crucial in transforming raw text into a clean, structured format suitable for modeling. The transformer model emerged as the best performer, achieving the highest F1-score (0.7736) and Kaggle F-score (0.80876), demonstrating its ability to capture complex patterns in text data. Logistic Regression and SVM also delivered strong results with F1-scores of 0.7525 and 0.7425, respectively, while being significantly faster to train, making them efficient alternatives for tasks with limited computational resources. CatBoost and LSTM provided moderate performance but required more training time, and Random Forest performed poorly, underscoring its unsuitability for high-dimensional text data. This work taught me the value of careful preprocessing, the trade-offs between model complexity and performance, and the importance of selecting the right model based on the task requirements and available resources. Overall, the transformer model, despite its longer training time, proved to be the optimal choice for this task due to its superior accuracy, recall, and generalization capabilities, while simpler models like Logistic Regression remain viable for faster, resource-efficient solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
