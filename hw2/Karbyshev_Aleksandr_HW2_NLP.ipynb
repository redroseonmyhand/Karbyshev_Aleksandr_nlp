{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, set_seed\n",
    "import math\n",
    "import random\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Ag News Dataset in my work. Source of the dataset -  AG's News Corpus. There are short news article in English from 4 categories: world, sport, business, science/technology.\n",
    "\n",
    "I took 5000 texts from dataset to make calculations faster. The average length of text is 45.58 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 5000\n",
      "Text Example:\n",
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"ag_news\", split=\"train[:5000]\")\n",
    "\n",
    "texts = [x[\"text\"] for x in dataset]\n",
    "\n",
    "# Text Example\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "print(\"Text Example:\")\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text: 45.58 tokens\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "tokenized = [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "all_tokens = [token for text in tokenized for token in text]\n",
    "\n",
    "lengths = [len(toks) for toks in tokenized]\n",
    "\n",
    "avg_len = np.mean(lengths)\n",
    "\n",
    "print(f\"Average length of text: {avg_len:.2f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Trigram Language Model\n",
    "\n",
    "Then I trained a statistical n-gram language model using trigrams (n=3). The model is based on frequency counts of word sequences from a tokenized text corpus. For each pair of consecutive words `(w1, w2)`, it collects statistics about which words `w3` tend to follow, and how frequently.\n",
    "\n",
    "### Chosen Hyperparameters\n",
    "\n",
    "- **n = 3 (Trigram model):** I use a context window of two words to predict the third. This balances complexity and data sparsity: bigram models (n=2) may be too simple, while higher-order models (n > 3) require much more data to be effective.\n",
    "- **Tokenization (implicit):** The model relies on pre-tokenized input (`all_tokens`).\n",
    "\n",
    "This trigram model serves as a basic foundation for more advanced language modeling tasks such as text generation or autocomplete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list(ngrams(all_tokens, 3))\n",
    "\n",
    "model = defaultdict(list)\n",
    "for w1, w2, w3 in trigrams:\n",
    "    model[(w1, w2)].append(w3)\n",
    "\n",
    "\n",
    "prob_model = defaultdict(Counter)\n",
    "for w1, w2, w3 in trigrams:\n",
    "    prob_model[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Language Model Training\n",
    "\n",
    "In this section I implemented a language model using an LSTM (Long Short-Term Memory) neural network. The model is trained to predict the next word in a sequence given the previous words, based on a sliding window of fixed size over tokenized text data.\n",
    "\n",
    "### Model Description\n",
    "\n",
    "The model processes sequences of 4 input words and predicts the 5th word. Each word is first converted to an index using a vocabulary mapping, and then passed through an embedding layer followed by an LSTM and a linear output layer. The final prediction is a probability distribution over the vocabulary.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- **Embedding Layer:** Converts word indices into dense vectors of fixed size (100 dimensions).\n",
    "- **LSTM Layer:** Processes sequences of embeddings with a hidden state size of 128. The `batch_first=True` argument allows input tensors to be shaped as (batch, sequence, features).\n",
    "- **Fully Connected (Linear) Layer:** Maps the LSTM's last hidden state to vocabulary size logits, used for classification via cross-entropy loss.\n",
    "\n",
    "### Chosen Hyperparameters\n",
    "\n",
    "- **Sequence length:** 5 tokens (4 input tokens + 1 target token).\n",
    "- **Embedding dimension:** 100 — balances representational capacity with training speed.\n",
    "- **Hidden size of LSTM:** 128 — a common default that offers sufficient modeling capacity for moderately complex tasks.\n",
    "- **Batch size:** 64 — chosen for stable training and efficiency on typical hardware.\n",
    "- **Learning rate:** 0.001 — standard for the Adam optimizer.\n",
    "- **Loss function:** CrossEntropyLoss — suitable for multi-class classification over vocabulary.\n",
    "- **Epochs:** 3 — enough for demonstration or early convergence on small datasets.\n",
    "\n",
    "### Vocabulary\n",
    "\n",
    "- Special tokens include:\n",
    "  - `<PAD>` (index 0) — for padding sequences if necessary.\n",
    "  - `<UNK>` (index 1) — for unknown or out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(all_tokens)\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(vocab.items())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "inv_vocab = {i: w for w, i in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "sequences = []\n",
    "seq_length = 5\n",
    "\n",
    "for tokens in tokenized:\n",
    "    indexed = [vocab.get(w, 1) for w in tokens]\n",
    "    for i in range(len(indexed) - seq_length):\n",
    "        seq = indexed[i:i+seq_length]\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.x = [torch.tensor(seq[:-1]) for seq in sequences]\n",
    "        self.y = [torch.tensor(seq[-1]) for seq in sequences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "dataset = TextDataset(sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "model_lstm = LSTMModel(vocab_size=vocab_size, embedding_dim=100, hidden_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 4,364,336\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Trainable parameters: {count_parameters(model_lstm):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.8697\n",
      "Loss: 4.3741\n",
      "Loss: 4.7703\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_lstm.train()\n",
    "for epoch in range(3):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_lstm(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning a Pretrained Language Model (DistilGPT2)\n",
    "\n",
    "Then I fine-tuned a pretrained causal language model — specifically `distilgpt2` — using a custom text corpus. The model is part of the Hugging Face `transformers` library and was originally trained in a self-supervised manner to predict the next token in a sequence (causal language modeling).\n",
    "\n",
    "### Model Description: DistilGPT2\n",
    "\n",
    "- **Architecture:** DistilGPT2 is a distilled version of OpenAI's GPT-2 model. It uses the same Transformer decoder architecture as GPT-2 but with fewer layers and parameters.\n",
    "- **Pretraining Data:** DistilGPT2 was pretrained on a large subset of OpenWebText, which is a filtered and cleaned version of web pages linked from Reddit with high karma scores.\n",
    "- **Number of Layers:** 6 Transformer decoder blocks (compared to 12 in GPT2-base).\n",
    "- **Hidden Size:** 768\n",
    "- **Attention Heads:** 12\n",
    "- **Total Parameters:** ~82 million\n",
    "- **Tokenizer:** Byte-Pair Encoding (BPE) tokenizer derived from GPT-2’s original tokenizer.\n",
    "- **Special Tokens:** The padding token is set manually to the end-of-sequence (EOS) token, as the original GPT-2 models do not use padding by default.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "- Tokenization is performed using the pretrained `distilgpt2` tokenizer with:\n",
    "  - `max_length = 128` — truncates longer texts to a fixed maximum length.\n",
    "  - `truncation=True` — ensures that inputs longer than the limit are truncated instead of causing errors.\n",
    "- The dataset is then tokenized and formatted for language modeling (causal, not masked) using `DataCollatorForLanguageModeling`.\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "- **Epochs:** 1 — fine-tuning is kept minimal for demonstration purposes or due to compute constraints.\n",
    "- **Batch size:** 2 (per device) — small batch size likely due to memory limitations.\n",
    "- **Save steps:** 500 — checkpoints are saved periodically.\n",
    "- **Logging steps:** 100 — training progress is logged frequently.\n",
    "- **No MLM (Masked Language Modeling):** Causal LM is used (`mlm=False`), appropriate for GPT-style models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4873b128686643c3b475c78e3a756136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2bbb3ad96d442ea3b582de8014d87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 1:12:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.236800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.896100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.881400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.753400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.680500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.516400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.667600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.704200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.634000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=3.7405161865234375, metrics={'train_runtime': 4381.4381, 'train_samples_per_second': 1.141, 'train_steps_per_second': 0.571, 'total_flos': 82843828568064.0, 'train_loss': 3.7405161865234375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"ag_news_small.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in texts:\n",
    "        f.write(line.strip().replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
    "model_gpt = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"ag_news_small.txt\"})\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer_gpt(example[\"text\"], truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_gpt, mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_gpt,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_phrases = [\n",
    "    \"The stock market rose sharply today\",\n",
    "    \"A new study shows promising results\",\n",
    "    \"Scientists discovered a new planet\",\n",
    "    \"The president gave a speech in Washington\",\n",
    "    \"The football team won the championship\"\n",
    "]\n",
    "\n",
    "incorrect_phrases = [\n",
    "    \"Banana flies reading homework loudly\",\n",
    "    \"The rocket sandwiches eleven purple dreams\",\n",
    "    \"Computers dance above the microwave\",\n",
    "    \"Yesterday the dog painted democracy slowly\",\n",
    "    \"Sky jellyphones defeat politics under cheese\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_perplexity_laplace(phrase, prob_model, vocab, n=3):\n",
    "    tokens = word_tokenize(phrase.lower())\n",
    "    trigrams = list(ngrams(tokens, n))\n",
    "    log_prob = 0\n",
    "    V = len(vocab)\n",
    "    N = len(trigrams)\n",
    "    for gram in trigrams:\n",
    "        context = gram[:-1]\n",
    "        word = gram[-1]\n",
    "        next_words = prob_model.get(context, {})\n",
    "        count = next_words.get(word, 0)\n",
    "        total = sum(next_words.values())\n",
    "        prob = (count + 1) / (total + V)\n",
    "        log_prob += math.log(prob)\n",
    "    return math.exp(-log_prob / N) if N > 0 else float('inf')\n",
    "\n",
    "\n",
    "def lstm_perplexity(phrase, model, vocab, device='cpu'):\n",
    "    model.eval()\n",
    "    tokens = word_tokenize(phrase.lower())\n",
    "    token_ids = [vocab.get(t, 1) for t in tokens]\n",
    "    N = len(token_ids) - 1\n",
    "    if N <= 0:\n",
    "        return float('inf')\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for i in range(N):\n",
    "            input_seq = torch.tensor(token_ids[i:i+4]).unsqueeze(0)  # (1, 4)\n",
    "            target = torch.tensor([token_ids[i+4]]) if i+4 < len(token_ids) else None\n",
    "            if target is None:\n",
    "                break\n",
    "            output = model(input_seq)\n",
    "            loss = loss_fn(output, target)\n",
    "            losses.append(loss.item())\n",
    "        mean_loss = sum(losses) / len(losses) if losses else float('inf')\n",
    "        return math.exp(mean_loss)\n",
    "def calculate_perplexity(model, tokenizer, sentence):\n",
    "    encodings = tokenizer(sentence, return_tensors='pt')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "    return math.exp(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Perplexity (N-gram + Laplace) ==\n",
      "[CORRECT] The stock market rose sharply today -> 10436.07\n",
      "[CORRECT] A new study shows promising results -> 13142.02\n",
      "[CORRECT] Scientists discovered a new planet -> 12896.19\n",
      "[CORRECT] The president gave a speech in Washington -> 18545.40\n",
      "[CORRECT] The football team won the championship -> 18552.50\n",
      "[INCORRECT] Banana flies reading homework loudly -> 18544.00\n",
      "[INCORRECT] The rocket sandwiches eleven purple dreams -> 18544.25\n",
      "[INCORRECT] Computers dance above the microwave -> 18545.33\n",
      "[INCORRECT] Yesterday the dog painted democracy slowly -> 18544.00\n",
      "[INCORRECT] Sky jellyphones defeat politics under cheese -> 18544.00\n",
      "\n",
      "== Perplexity (LSTM) ==\n",
      "[CORRECT] The stock market rose sharply today -> 1018.98\n",
      "[CORRECT] A new study shows promising results -> 26866.00\n",
      "[CORRECT] Scientists discovered a new planet -> 17457.32\n",
      "[CORRECT] The president gave a speech in Washington -> 756.26\n",
      "[CORRECT] The football team won the championship -> 202.27\n",
      "[INCORRECT] Banana flies reading homework loudly -> 130478862.93\n",
      "[INCORRECT] The rocket sandwiches eleven purple dreams -> 115185.68\n",
      "[INCORRECT] Computers dance above the microwave -> 35808157.72\n",
      "[INCORRECT] Yesterday the dog painted democracy slowly -> 622641.87\n",
      "[INCORRECT] Sky jellyphones defeat politics under cheese -> 15137.29\n",
      "== Perplexity (Fine-tuned GPT-2) ==\n",
      "[CORRECT] The stock market rose sharply today -> 133.71\n",
      "[CORRECT] A new study shows promising results -> 292.93\n",
      "[CORRECT] Scientists discovered a new planet -> 84.61\n",
      "[CORRECT] The president gave a speech in Washington -> 54.49\n",
      "[CORRECT] The football team won the championship -> 158.59\n",
      "[INCORRECT] Banana flies reading homework loudly -> 26844.95\n",
      "[INCORRECT] The rocket sandwiches eleven purple dreams -> 123535.66\n",
      "[INCORRECT] Computers dance above the microwave -> 1566.19\n",
      "[INCORRECT] Yesterday the dog painted democracy slowly -> 15522.77\n",
      "[INCORRECT] Sky jellyphones defeat politics under cheese -> 45892.39\n"
     ]
    }
   ],
   "source": [
    "print(\"== Perplexity (N-gram + Laplace) ==\")\n",
    "for phrase in correct_phrases:\n",
    "    print(f\"[CORRECT] {phrase} -> {ngram_perplexity_laplace(phrase, prob_model, vocab):.2f}\")\n",
    "for phrase in incorrect_phrases:\n",
    "    print(f\"[INCORRECT] {phrase} -> {ngram_perplexity_laplace(phrase, prob_model, vocab):.2f}\")\n",
    "\n",
    "print(\"\\n== Perplexity (LSTM) ==\")\n",
    "for phrase in correct_phrases:\n",
    "    print(f\"[CORRECT] {phrase} -> {lstm_perplexity(phrase, model_lstm, vocab):.2f}\")\n",
    "for phrase in incorrect_phrases:\n",
    "    print(f\"[INCORRECT] {phrase} -> {lstm_perplexity(phrase, model_lstm, vocab):.2f}\")\n",
    "\n",
    "print(\"== Perplexity (Fine-tuned GPT-2) ==\")\n",
    "for phrase in correct_phrases:\n",
    "    ppl = calculate_perplexity(model_gpt, tokenizer_gpt, phrase)\n",
    "    print(f\"[CORRECT] {phrase} -> {ppl:.2f}\")\n",
    "for phrase in incorrect_phrases:\n",
    "    ppl = calculate_perplexity(model_gpt, tokenizer_gpt, phrase)\n",
    "    print(f\"[INCORRECT] {phrase} -> {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Language Models with Perplexity\n",
    "\n",
    "In this section, I evaluated the quality of three different language models — N-gram with Laplace smoothing, LSTM, and fine-tuned GPT-2 — using the perplexity metric. I used a set of 5 grammatically and semantically correct phrases and 5 incorrect or nonsensical phrases to compare how well each model distinguishes coherent language from incoherent sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### What is Perplexity?\n",
    "\n",
    "Perplexity is a widely used metric for evaluating language models. It measures how well a probability model predicts a sample.\n",
    "\n",
    "- Lower perplexity indicates that the model assigns higher probability to the test sequence — implying better performance.\n",
    "- Higher perplexity means the model finds the sequence surprising or unlikely.\n",
    "\n",
    "---\n",
    "\n",
    "### Results Overview\n",
    "\n",
    "#### Correct Phrases\n",
    "\n",
    "| Sentence                                         | N-gram    | LSTM        | GPT-2         |\n",
    "|--------------------------------------------------|-----------|-------------|---------------|\n",
    "| The stock market rose sharply today              | 10436.07  | 1018.98     | 133.71        |\n",
    "| A new study shows promising results              | 13142.02  | 26866.00    | 292.93        |\n",
    "| Scientists discovered a new planet               | 12896.19  | 17457.32    | 84.61         |\n",
    "| The president gave a speech in Washington        | 18545.40  | 756.26      | 54.49         |\n",
    "| The football team won the championship           | 18552.50  | 202.27      | 158.59        |\n",
    "\n",
    "#### Incorrect Phrases\n",
    "\n",
    "| Sentence                                              | N-gram    | LSTM          | GPT-2         |\n",
    "|-------------------------------------------------------|-----------|---------------|---------------|\n",
    "| Banana flies reading homework loudly                  | 18544.00  | 130M+         | 26,844.95      |\n",
    "| The rocket sandwiches eleven purple dreams            | 18544.25  | 115,185.68    | 123,535.66     |\n",
    "| Computers dance above the microwave                   | 18545.33  | 35M+          | 1,566.19       |\n",
    "| Yesterday the dog painted democracy slowly            | 18544.00  | 622,641.87    | 15,522.77      |\n",
    "| Sky jellyphones defeat politics under cheese          | 18544.00  | 15,137.29     | 45,892.39      |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation and Comparison\n",
    "\n",
    "#### N-gram Model\n",
    "- Shows little variation in perplexity between correct and incorrect sentences.\n",
    "- This indicates the model lacks sensitivity to semantic coherence — it is primarily counting word co-occurrences without deeper understanding.\n",
    "- High perplexity values overall suggest limited predictive power due to data sparsity and lack of context modeling.\n",
    "\n",
    "#### LSTM Model\n",
    "- Shows strong differentiation: perplexity is much lower on correct phrases than incorrect ones.\n",
    "- However, the perplexity values for some incorrect sentences are extremely high (e.g., 130M), indicating instability or sensitivity to out-of-distribution inputs.\n",
    "- Suggests the model can learn sequential dependencies but may overfit or behave erratically outside training distribution.\n",
    "\n",
    "#### Fine-tuned GPT-2\n",
    "- Achieves the lowest perplexity on correct phrases and consistently high perplexity on incorrect ones, but without extreme outliers.\n",
    "- Demonstrates the best balance of fluency recognition and stability.\n",
    "- Fine-tuning allows the model to better capture domain-specific language and semantics.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Perplexity is a valuable tool for evaluating how well a language model understands and predicts text. In this comparison:\n",
    "- GPT-2 (fine-tuned) offers the most reliable and accurate judgments about sentence plausibility.\n",
    "- LSTM also distinguishes well but is less stable.\n",
    "- N-gram model fails to differentiate between plausible and nonsensical text due to its shallow nature.\n",
    "\n",
    "These findings emphasize the importance of using deep, pretrained models and domain-specific fine-tuning for high-quality language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_phrases = [\n",
    "    \"I think\",\n",
    "    \"The government\",\n",
    "    \"Scientists believe\",\n",
    "    \"He said that\",\n",
    "    \"Blue dog\",\n",
    "    \"Today we\",\n",
    "    \"The weather\",\n",
    "    \"This research\",\n",
    "    \"According to\",\n",
    "    \"She goes to\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I think': 'i think possible the same subs , law . hatem abdel kader , as tiny ear bones indicates that consumers and silicon', 'The government': 'the government partially agreed to\\\\be on easing fears about 28,000 batteries the race ( r66 billion punta gorda , pressured and derail', 'Scientists believe': 'scientists believe was apparently suffering a comet ... nicky hilton . murdoch was further cement its auto-update servers . bird flu season', 'He said that': \"he said that four members on event at new messagelabs report ( computerworld 's official baghdad opened the waiter appears they washed out\", 'Blue dog': 'blue dog new semiconductor starts at bungalow billiards in ohio . europe is near silvio berlusconi met survivors and eolas patent infringement', 'Today we': 'today we look phishy ? # 225 ; can # 133 ; emulation with eight-run eighth inning and do things up 129', 'The weather': 'the weather pushed an oval west darfur troop cut news site shows a villa , sc hurricane estimate on palestinian boy has', 'This research': 'this research and\\\\development spending . twenty-five tap-air portugal after billionaire investor sentiment ... treasuries up more skilled at baghdad that in southern', 'According to': 'according to task is undergoing shoulder surgery ( update1 ) pc vulnerabilities to whale fossils show people seeking work ... americans miss', 'She goes to': 'she goes to wait a ripple effect ( initial order them around 3,500 ... wireless networks plans , vcard support , wound up'}\n",
      "['i think of the world # 39 ; s new york ( reuters ) reuters - the united states , the world', 'the government to the world # 39 ; s new york ( reuters ) reuters - the united states , the world', 'scientists believe the world # 39 ; s new york ( reuters ) reuters - the united states , the world #', 'he said that it has been found in the united states , the world # 39 ; s top sprinters kostas kenteris and', 'blue dog in the men # 39 ; s new york ( reuters ) reuters - the united states , the world', \"today we the company 's initial public offering , the world # 39 ; s new york ( reuters ) reuters -\", 'the weather of the world # 39 ; s new york ( reuters ) reuters - the united states , the world', \"this research ) on tuesday , but the company 's initial public offering , the world # 39 ; s new york\", 'according to a new species , and the world # 39 ; s new york ( reuters ) reuters - the united', 'she goes to be a new species of flightless bird flu vaccine to the united states , the world # 39 ; s']\n",
      "{'I think': 'I think John McCain will never win -- I think the McCain will win in my view is very sad and sad', 'The government': 'The government confirms no record of a terrorist attack in Greece In a statement, the US administration said only three terrorists', 'Scientists believe': \"Scientists believe that the government's initial funding isn't too much needed, as the Federal Housing Agency (FHA\", 'He said that': 'He said that in all its glory the Yankees have made him the greatest of his career. The World Series champion said', 'Blue dog': 'Blue dog with dog leash The Canadian Press - A Toronto dog dog has been poached and strangled for weeks', 'Today we': \"Today we're getting too busy with these movies  ATHENS - ATHENS, Greece - Film companies\", 'The weather': 'The weather forces forecast ATHENS, Greece (Reuters) - Greece is heading for a  chilly week with', 'This research': \"This research suggests women of this size' are not more likely to be in the hospital at the time of the\", 'According to': 'According to Make Up for (Update1) This means the US dollar might do the same for Europe this week', 'She goes to': 'She goes to the hospital, gets a flu vaccine I am in the hospital and is in the hospital. My dad'}\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "def generate_with_gpt2(model, tokenizer, prompts, max_new_tokens=20):\n",
    "    gpt2_outputs = {}\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, top_k=50)\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        gpt2_outputs[prompt] = text\n",
    "    return gpt2_outputs\n",
    "\n",
    "def generate_with_lstm(model, tokenizer, start_texts, idx_to_word, word_to_idx, max_len=20):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for prompt in start_texts:\n",
    "        words = tokenizer(prompt.lower())\n",
    "        input_seq = [word_to_idx.get(w, word_to_idx[\"<UNK>\"]) for w in words[-3:]]\n",
    "        generated = words.copy()\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            x = torch.tensor(input_seq).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                output = model(x)\n",
    "                predicted_id = output.argmax(dim=-1).item()\n",
    "\n",
    "            predicted_word = idx_to_word.get(predicted_id, \"<UNK>\")\n",
    "            generated.append(predicted_word)\n",
    "\n",
    "            input_seq = input_seq[1:] + [predicted_id]\n",
    "\n",
    "        results.append(\" \".join(generated))\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_with_ngram(ngram_model, tokenizer, prompts, max_len=20):\n",
    "    ngram_outputs = {}\n",
    "    for prompt in prompts:\n",
    "        tokens = tokenizer(prompt.lower())\n",
    "        generated = tokens.copy()\n",
    "        for _ in range(max_len):\n",
    "            context = tuple(generated[-2:])\n",
    "            candidates = [w for w in ngram_model if w[:-1] == context]\n",
    "            if not candidates:\n",
    "                context = tuple(generated[-1:])\n",
    "                candidates = [w for w in ngram_model if w[:-1] == context]\n",
    "            if not candidates:\n",
    "                break\n",
    "            next_word = random.choice(candidates)[-1]\n",
    "            generated.append(next_word)\n",
    "        ngram_outputs[prompt] = \" \".join(generated)\n",
    "    return ngram_outputs\n",
    "\n",
    "ngram_gen = generate_with_ngram(prob_model, word_tokenize, start_phrases)\n",
    "lstm_gen = generate_with_lstm(model_lstm, word_tokenize, start_phrases, inv_vocab, vocab)\n",
    "gpt2_gen = generate_with_gpt2(model_gpt, tokenizer_gpt, start_phrases)\n",
    "\n",
    "print(ngram_gen)\n",
    "print(lstm_gen)\n",
    "print(gpt2_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation from Different Starting Phrases: Model Comparison\n",
    "\n",
    "In this section, I generated continuations for a set of 10 different starting phrases using three language models.\n",
    "\n",
    "These phrases serve as prompts to test how coherently and contextually each model continues text. The goal is to evaluate fluency, coherence, grammaticality, and relevance of generated outputs. \n",
    "\n",
    "---\n",
    "\n",
    "### N-gram Model\n",
    "\n",
    "- **Characteristics**:\n",
    "  - Disjointed and fragmented structure.\n",
    "  - No clear grammatical or semantic coherence.\n",
    "  - Often mixes unrelated noun phrases without proper connectors.\n",
    "- **Interpretation**: N-gram models fail to maintain context across more than 2–3 tokens. They lack long-range dependencies and often produce incoherent, unnatural sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### LSTM Model\n",
    "\n",
    "- **Characteristics**:\n",
    "  - Better sentence structure and fluency than n-gram.\n",
    "  - Tendency to repeat common training patterns (e.g., news-like phrasing from Reuters data).\n",
    "  - Overuses clichés and boilerplate text.\n",
    "- **Interpretation**: LSTM models capture some level of global structure but are limited by training data size and sequence length. Outputs are more fluent but can be generic or repetitive.\n",
    "\n",
    "---\n",
    "\n",
    "### Fine-tuned GPT-2\n",
    "\n",
    "- **Characteristics**:\n",
    "  - Fluent and grammatically correct.\n",
    "  - Capable of producing topic-consistent and human-like continuations.\n",
    "  - Sometimes repeats or drifts slightly, but retains context more effectively.\n",
    "- **Interpretation**: GPT-2 excels in generating coherent, plausible, and grammatically accurate text. Fine-tuning helps it stay more relevant to the style and vocabulary of the target domain.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- The N-gram model is highly limited for generative tasks and produces mostly incoherent phrases.\n",
    "- The LSTM model performs moderately better, generating readable but often templated outputs.\n",
    "- The fine-tuned GPT-2 model consistently produces the most fluent, meaningful, and context-aware completions, confirming the strength of Transformer-based architectures in language generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Which Model Performed Best?\n",
    "\n",
    "Among the three evaluated models — N-gram, LSTM, and fine-tuned GPT-2 — the fine-tuned GPT-2 demonstrated the best overall performance in terms of:\n",
    "\n",
    "- Text generation quality: GPT-2 outputs were significantly more coherent, fluent, and contextually relevant.\n",
    "- Perplexity: GPT-2 consistently assigned lower perplexity to grammatically and semantically correct sentences, and much higher perplexity to nonsensical ones — indicating a strong ability to distinguish between good and bad language.\n",
    "\n",
    "So, GPT-2 is clearly the best in terms of output quality and perplexity accuracy, though at a cost of higher computational resources.\n",
    "\n",
    "---\n",
    "\n",
    "### How Can Results Be Improved?\n",
    "\n",
    "- **N-gram**:\n",
    "  - I can use higher-order n-grams (e.g., 4-grams or 5-grams).\n",
    "  - Also I can try Kneser-Ney smoothing instead of basic Laplace.\n",
    "  - Integrate back-off models to improve handling of sparse data.\n",
    "\n",
    "- **LSTM**:\n",
    "  - Increase training data volume and diversity.\n",
    "  - Try to use bidirectional LSTM or stacked LSTMs for deeper representations.\n",
    "  - Add regularization techniques (dropout, weight decay) to prevent overfitting.\n",
    "\n",
    "- **GPT-2**:\n",
    "  - Fine-tune for more epochs or with more data for stronger domain adaptation.\n",
    "  - Try to use larger GPT models (e.g., GPT-2 medium or large) if resources allow.\n",
    "  - Experiment with prompt engineering to guide generation behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### Difficulties Encountered\n",
    "\n",
    "- **N-gram limitations**: Very sensitive to vocabulary size and sparsity. Unable to generalize or maintain long-term context.\n",
    "- **LSTM issues**:\n",
    "  - Requires careful tuning of sequence length, batch size, and learning rate.\n",
    "  - Sometimes produces repetitive or generic outputs.\n",
    "- **GPT-2 challenges**:\n",
    "  - High memory and compute requirements.\n",
    "  - Fine-tuning requires careful tokenization and format consistency.\n",
    "  - Susceptible to overfitting on small datasets or generating biased content if training data isn't well-balanced.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "Each model serves a different purpose. While simpler models like N-grams are easy to implement and fast to train, modern Transformer-based models like GPT-2 offer vastly superior performance for both perplexity evaluation and text generation — making them the preferred choice for high-quality NLP applications, especially when computational resources are available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
